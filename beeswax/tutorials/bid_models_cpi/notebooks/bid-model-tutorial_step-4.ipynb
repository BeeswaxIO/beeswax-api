{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a CPI Bid Model (Part 4)\n",
    "\n",
    "In Part 3, we built our fist model but we found that the initial performance was very poor.  The main reason for this is that the number of conversions is very small relative to the total number of impressions.  When we train the model with this data it does a really good job of predicting non-converters but a poor job of predicting converters.  In this part, we will address this problem and then take steps to further tune the performance of the model.  \n",
    "\n",
    "## Re-sampling\n",
    "To combat the relatively small number of conversions, we can [upsample](http://www.simafore.com/blog/handling-unbalanced-data-machine-learning-models) the training set so that the conversions are more evenly balanced with the non-conversions.  This will ultimately allow the model to train on a more evenly distributed dataset.  Our goal is to create a balanced dataset for training, and then to test on an unbalanced dataset to make sure the model still works in the real world.\n",
    "\n",
    "Let's start by re-loading our dataset from Step 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install joblib\n",
    "!{sys.executable} -m pip install graphviz\n",
    "!{sys.executable} -m pip install mxnet\n",
    "\n",
    "# our usual setup, also making sure some system libraries are installed\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import joblib\n",
    "import json\n",
    "import xgboost\n",
    "import matplotlib.pyplot as plt\n",
    "import mxnet as mx\n",
    "\n",
    "%matplotlib inline\n",
    "from sagemaker.analytics import TrainingJobAnalytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./data/step2-model.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the sampling.  We'll try to improve the ratio so that at least 5% of rows have some conversion rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gives us 100% of the data where conversion_rate > 0\n",
    "sampled_data = df.loc[df['conversion_rate'] > 0]\n",
    "\n",
    "# now let's calculate what percentage to sample\n",
    "sample_rate = len(sampled_data) * 20.0 / (len(df) - len(sampled_data))\n",
    "print('sample rate: {}'.format(sample_rate))\n",
    "\n",
    "# finally, let's merge the two together\n",
    "sampled_data = pd.concat([sampled_data, df.sample(frac=sample_rate)], axis=0, ignore_index=True).to_dense()\n",
    "print('new data size: {}'.format(sampled_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we have a much better ratio of converters to non-converters, and the added benefit of a much smaller dataset that will allow us to move much faster from here out.  This should give us a better fitting model.  Let's find out by re-running our modeling process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sampled_data.sample(frac=.9)\n",
    "validation_data = sampled_data.drop(train_data.index)\n",
    "\n",
    "pd.concat([train_data['conversion_rate'], train_data.drop(['conversion_rate'], axis=1)], axis=1).to_csv('data/sampled/train.csv', index=False, header=False)\n",
    "pd.concat([validation_data['conversion_rate'], validation_data.drop(['conversion_rate'], axis=1)], axis=1).to_csv('data/sampled/validation.csv', index=False, header=False)\n",
    "\n",
    "bucket = 'beeswax-tmp-us-east-1'\n",
    "prefix = 'bid-models-test-data/canary/sagemaker'\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'sampled/train/train.csv')).upload_file('data/sampled/train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'sampled/validation/validation.csv')).upload_file('data/sampled/validation.csv')\n",
    "\n",
    "# drop the data files from disk, they are huge and we don't want to keep them\n",
    "os.remove('data/sampled/train.csv')\n",
    "os.remove('data/sampled/validation.csv')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri(boto3.Session().region_name, 'linear-learner')\n",
    "\n",
    "bucket = 'beeswax-tmp-us-east-1'\n",
    "prefix = 'bid-models-test-data/canary/sagemaker/sampled'\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "ll = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.4xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=session)\n",
    "ll.set_hyperparameters(\n",
    "    feature_dim=len(sampled_data.columns)-1,\n",
    "    mini_batch_size=200,\n",
    "    predictor_type='regressor'\n",
    ")\n",
    "\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train/'.format(bucket, prefix), content_type='text/csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='text/csv')\n",
    "\n",
    "job_name = 'canary-cpi-model-sampled-{timestamp}'.format(timestamp=int(time.time()))\n",
    "ll.fit({'train': s3_input_train}, job_name=job_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate the model as before.  I'm going to use our original test dataset (without sampling) to make sure that our sampling didn't cause us to overfit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_pickle('./data/step2-test.pkl')\n",
    "\n",
    "ll_predictor = ll.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "ll_predictor.content_type = 'text/csv'\n",
    "ll_predictor.serializer = csv_serializer\n",
    "ll_predictor.deserializer = json_deserializer\n",
    "\n",
    "def predict(data):\n",
    "    predictions = []\n",
    "    for array in data:\n",
    "        result = ll_predictor.predict(array)\n",
    "        predictions.append(result['predictions'][0]['score'])\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "test_data['prediction'] = predict(test_data.drop(['conversion_rate'], axis=1).as_matrix())\n",
    "test_data['error'] = np.abs(test_data['prediction'] - test_data['conversion_rate'])\n",
    "print('\\nmean average error: {error}'.format(error=test_data['error'].mean()))\n",
    "print('mean average error for non-zero conversion rate: {error}'.format(error=test_data.loc[test_data['conversion_rate'] > 0]['error'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we've been able to balance our model a little better.  We are now worse at determining what does not perform well but better at determining what does perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "The next step we will take is to tune our hyperparameters (basically, the configuration for the model).  This can have a dramatic effect on the performace of the model.  The bad news is that really the only way to tune these parameters is to try a bunch of different combinations.  The good news is that SageMaker has a feature specifically for this process!\n",
    "\n",
    "We will tune four hyperparameters for our model:\n",
    "\n",
    "* `eta`: Step size shrinkage used in updates to prevent overfitting. After each boosting step, you can directly get the weights of new features. The eta parameter actually shrinks the feature weights to make the boosting process more conservative.\n",
    "* `alpha`: L1 regularization term on weights. Increasing this value makes models more conservative.\n",
    "* `min_child_weight`: Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, the building process gives up further partitioning. In linear regression models, this simply corresponds to a minimum number of instances needed in each node. The larger the algorithm, the more conservative it is.\n",
    "* `max_depth`: Maximum depth of a tree. Increasing this value makes the model more complex and likely to be overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "hyperparameter_ranges = {\n",
    "    'wd': ContinuousParameter(0.0001, 1),\n",
    "    'l1': ContinuousParameter(0.0001, 1),\n",
    "    'learning_rate': ContinuousParameter(0.0001, 1),\n",
    "    'mini_batch_size': IntegerParameter(100, 500),\n",
    "    'use_bias': CategoricalParameter([True, False]),\n",
    "    'positive_example_weight_mult': ContinuousParameter(0.0001, 1),\n",
    "}\n",
    "objective_metric_name = 'validation:objective_loss'\n",
    "\n",
    "tuner = HyperparameterTuner(ll, objective_metric_name, hyperparameter_ranges, objective_type='Minimize', max_jobs=30, max_parallel_jobs=5)\n",
    "job_name = 'canary-cpi-tuner-{timestamp}'.format(timestamp=int(time.time()))\n",
    "tuner.fit({'train': s3_input_train, 'validation': s3_input_validation}, include_cls_metadata=False, job_name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block until tuning finishes\n",
    "\n",
    "sage_client = boto3.client('sagemaker', region_name=boto3.Session().region_name)\n",
    "tuning_job_result = sage_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=job_name)\n",
    "while tuning_job_result['HyperParameterTuningJobStatus'] != 'Completed':\n",
    "    print('tuning in progress...')\n",
    "    time.sleep(60)\n",
    "    tuning_job_result = sage_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=job_name)\n",
    "\n",
    "tuned_hyper_params = tuning_job_result['BestTrainingJob']['TunedHyperParameters']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the completion of our tuning job (it will take quite some time), we want to pull the best job and retrain/redeploy our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri(boto3.Session().region_name, 'linear-learner')\n",
    "\n",
    "bucket = 'beeswax-tmp-us-east-1'\n",
    "prefix = 'bid-models-test-data/canary/sagemaker/sampled'\n",
    "\n",
    "session = sagemaker.Session()\n",
    "ll = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.4xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=session)\n",
    "ll.set_hyperparameters(\n",
    "    feature_dim=len(sampled_data.columns)-1,\n",
    "    predictor_type='regressor',\n",
    "    **tuned_hyper_params\n",
    ")\n",
    "\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train/'.format(bucket, prefix), content_type='text/csv')\n",
    "\n",
    "job_name = 'canary-cpi-model-sampled-{timestamp}'.format(timestamp=int(time.time()))\n",
    "ll.fit({'train': s3_input_train}, job_name=job_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then re-evaluate the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_pickle('./data/step2-test.pkl')\n",
    "\n",
    "ll_predictor = ll.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "ll_predictor.content_type = 'text/csv'\n",
    "ll_predictor.serializer = csv_serializer\n",
    "ll_predictor.deserializer = json_deserializer\n",
    "\n",
    "def predict(data):\n",
    "    predictions = []\n",
    "    for array in data:\n",
    "        result = ll_predictor.predict(array)\n",
    "        predictions.append(result['predictions'][0]['score'])\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "test_data['prediction'] = predict(test_data.drop(['conversion_rate'], axis=1).as_matrix())\n",
    "test_data['error'] = np.abs(test_data['prediction'] - test_data['conversion_rate'])\n",
    "print('mean average error: {error}'.format(error=test_data['error'].mean()))\n",
    "print('mean average error for non-zero conversion rate: {error}'.format(error=test_data.loc[test_data['conversion_rate'] > 0]['error'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that our starting hyperparameters were already pretty good and we didn't really improve much with tuning.  We'll keep this step in as we might see different results overtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Inspection\n",
    "One last model tuning step we'll look into is the relative performance and value of features in our model.  In general, we want to simplify our model as much as possible so that it's easy to understand and can be as concise as possible.  We may also be able to get better performance from the model by eliminating certain features that are confusing the model training process and ultimately doing more harm than good.\n",
    "\n",
    "Let's start by inspecting our existing model and determining how much impact each feature has on the final prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mxnet as mx\n",
    "import boto3\n",
    "\n",
    "key = '{}/output/{}/output/model.tar.gz'.format(prefix, jobname)\n",
    "\n",
    "boto3.resource('s3').Bucket(bucket).download_file(key, './data/model.tar.gz')\n",
    "os.system('tar -zxvf ./data/model.tar.gz')\n",
    "\n",
    "# Linear learner model is itself a zip file, containing a mxnet model and other metadata.\n",
    "# First unzip the model.\n",
    "os.system('unzip model_algo-1')\n",
    "\n",
    "# Load the mxnet module\n",
    "mod = mx.module.Module.load(\"mx-mod\", 0)\n",
    "\n",
    "# model weights\n",
    "weights = mod._arg_params['fc0_weight'].asnumpy().flatten()\n",
    "\n",
    "# merge the weights in with the feature labels\n",
    "model_weights = pd.DataFrame({'feature': list(sampled_data.columns[1:])})\n",
    "model_weights.head()\n",
    "model_weights['weights'] = weights\n",
    "\n",
    "model_weights.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the model coefficients for each feature, but remember that we one-hot encoded our data so we have a ton of features (~500).  It will be too difficult to look across all of these features so let's instead aggregate based on the original feature.  Basically, we'll reverse our one hot encoding and then compute some stats for each top level feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_level_features = set([col.split('-')[0] for col in sampled_data.columns[1:]])\n",
    "top_level_weights = pd.DataFrame({'features': list(top_level_features)})\n",
    "\n",
    "mean_weight = []\n",
    "med_weight = []\n",
    "min_weight = []\n",
    "max_weight = []\n",
    "dev_weight = []\n",
    "for feature in top_level_weights['features']:\n",
    "    _weights = model_weights.loc[model_weights['feature'].str.startswith(feature)]\n",
    "    mean_weight.append(_weights['weights'].abs().mean())\n",
    "    med_weight.append(_weights['weights'].abs().median())\n",
    "    min_weight.append(_weights['weights'].abs().min())\n",
    "    max_weight.append(_weights['weights'].abs().max())\n",
    "    dev_weight.append(_weights['weights'].std())\n",
    "\n",
    "top_level_weights['mean'] = mean_weight\n",
    "top_level_weights['med'] = med_weight\n",
    "top_level_weights['min'] = min_weight\n",
    "top_level_weights['max'] = max_weight\n",
    "top_level_weights['stdev'] = dev_weight\n",
    "\n",
    "top_level_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now calculated some descriptive stats for each feature.  Let's sort our features by median weight and graph them so we can see the relative importance of each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_level_weights = top_level_weights.sort_values(['med'])\n",
    "ax = top_level_weights.plot.barh(x='features', y='med', rot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are three features which have a very very small median impact on the model: `inventory_source`, `app_bundle`, and `platform_device_make`.  Let's try re-training our model without these features and see if our accuracy improves.  First, we'll need to re-generate our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = sampled_data\n",
    "model_df = model_df[[col for col in model_df if not col.startswith('inventory_source')]]\n",
    "model_df = model_df[[col for col in model_df if not col.startswith('app_bundle')]]\n",
    "model_df = model_df[[col for col in model_df if not col.startswith('platform_device_make')]]\n",
    "\n",
    "train_data = model_df.sample(frac=.7).to_dense()\n",
    "validation_data = model_df.drop(train_data.index).sample(frac=.66).to_dense()\n",
    "test_data = model_df.drop(train_data.index).drop(validation_data.index).to_dense()\n",
    "\n",
    "pd.concat([train_data['conversion_rate'], train_data.drop(['conversion_rate'], axis=1)], axis=1).to_csv('data/slim/train.csv', index=False, header=False)\n",
    "pd.concat([validation_data['conversion_rate'], validation_data.drop(['conversion_rate'], axis=1)], axis=1).to_csv('data/slim/validation.csv', index=False, header=False)\n",
    "\n",
    "bucket = 'beeswax-tmp-us-east-1'\n",
    "prefix = 'bid-models-test-data/canary/sagemaker'\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'slim/train/train.csv')).upload_file('data/slim/train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'slim/validation/validation.csv')).upload_file('data/slim/validation.csv')\n",
    "\n",
    "# drop the data files from disk, they are huge and we don't want to keep them\n",
    "os.remove('data/slim/train.csv')\n",
    "os.remove('data/slim/validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's retrain and re-test the model with the slimmed down dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri(boto3.Session().region_name, 'linear-learner')\n",
    "\n",
    "bucket = 'beeswax-tmp-us-east-1'\n",
    "prefix = 'bid-models-test-data/canary/sagemaker/slim'\n",
    "\n",
    "session = sagemaker.Session()\n",
    "ll = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.4xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=session)\n",
    "ll.set_hyperparameters(\n",
    "    feature_dim=len(train_data.columns)-1,\n",
    "    mini_batch_size=200,\n",
    "    predictor_type='regressor'\n",
    ")\n",
    "\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train/'.format(bucket, prefix), content_type='text/csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='text/csv')\n",
    "\n",
    "job_name = 'canary-cpi-model-slim-{timestamp}'.format(timestamp=int(time.time()))\n",
    "ll.fit({'train': s3_input_train, 'validation': s3_input_validation}, job_name=job_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_predictor = ll.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "ll_predictor.content_type = 'text/csv'\n",
    "ll_predictor.serializer = csv_serializer\n",
    "ll_predictor.deserializer = json_deserializer\n",
    "\n",
    "def predict(data):\n",
    "    predictions = []\n",
    "    for array in data:\n",
    "        result = ll_predictor.predict(array)\n",
    "        predictions.append(result['predictions'][0]['score'])\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "test_data['prediction'] = predict(test_data.drop(['conversion_rate'], axis=1).as_matrix())\n",
    "test_data['error'] = np.abs(test_data['prediction'] - test_data['conversion_rate'])\n",
    "print('\\nmean average error: {error}'.format(error=test_data['error'].mean()))\n",
    "print('mean average error for non-zero conversion rate: {error}'.format(error=test_data.loc[test_data['conversion_rate'] > 0]['error'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so our performance actually got worse when we removed these fields. Re-examining the table above, we see that some of the features we eliminated actually have relatively high \"max\" weights.  In other words, there are a few `app_bundle`s, etc, that do have significant impact on the model.  What if we instead look for low-coefficient features that also have low standard deviations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_level_weights = top_level_weights.sort_values(['stdev'], ascending=False)\n",
    "ax = top_level_weights.plot.barh(x='features', y=['stdev', 'med'], rot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we actually get a different set of low performing features this way.  Let's try removing the 5 lowest features (we'll leave `rewarded` in there since it has a failry high median value) instead and then re-training one more time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = sampled_data\n",
    "model_df = model_df[[col for col in model_df if not col.startswith('inventory_interstitial')]]\n",
    "model_df = model_df[[col for col in model_df if not col.startswith('banner_height')]]\n",
    "model_df = model_df[[col for col in model_df if not col.startswith('placement_type')]]\n",
    "model_df = model_df[[col for col in model_df if not col.startswith('platform_bandwidth')]]\n",
    "model_df = model_df[[col for col in model_df if not col.startswith('banner_width')]]\n",
    "\n",
    "train_data = model_df.sample(frac=.7).to_dense()\n",
    "validation_data = model_df.drop(train_data.index).sample(frac=.66).to_dense()\n",
    "test_data = model_df.drop(train_data.index).drop(validation_data.index).to_dense()\n",
    "\n",
    "pd.concat([train_data['conversion_rate'], train_data.drop(['conversion_rate'], axis=1)], axis=1).to_csv('data/slim/train.csv', index=False, header=False)\n",
    "pd.concat([validation_data['conversion_rate'], validation_data.drop(['conversion_rate'], axis=1)], axis=1).to_csv('data/slim/validation.csv', index=False, header=False)\n",
    "\n",
    "bucket = 'beeswax-tmp-us-east-1'\n",
    "prefix = 'bid-models-test-data/canary/sagemaker'\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'slim/train/train.csv')).upload_file('data/slim/train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'slim/validation/validation.csv')).upload_file('data/slim/validation.csv')\n",
    "\n",
    "# drop the data files from disk, they are huge and we don't want to keep them\n",
    "os.remove('data/slim/train.csv')\n",
    "os.remove('data/slim/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri(boto3.Session().region_name, 'linear-learner')\n",
    "\n",
    "bucket = 'beeswax-tmp-us-east-1'\n",
    "prefix = 'bid-models-test-data/canary/sagemaker/slim'\n",
    "\n",
    "session = sagemaker.Session()\n",
    "ll = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.4xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=session)\n",
    "ll.set_hyperparameters(\n",
    "    feature_dim=len(train_data.columns)-1,\n",
    "    mini_batch_size=200,\n",
    "    predictor_type='regressor'\n",
    ")\n",
    "\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train/'.format(bucket, prefix), content_type='text/csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='text/csv')\n",
    "\n",
    "job_name = 'canary-cpi-model-slim-{timestamp}'.format(timestamp=int(time.time()))\n",
    "ll.fit({'train': s3_input_train, 'validation': s3_input_validation}, job_name=job_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_predictor = ll.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "ll_predictor.content_type = 'text/csv'\n",
    "ll_predictor.serializer = csv_serializer\n",
    "ll_predictor.deserializer = json_deserializer\n",
    "\n",
    "def predict(data):\n",
    "    predictions = []\n",
    "    for array in data:\n",
    "        result = ll_predictor.predict(array)\n",
    "        predictions.append(result['predictions'][0]['score'])\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "test_data['prediction'] = predict(test_data.drop(['conversion_rate'], axis=1).as_matrix())\n",
    "test_data['error'] = np.abs(test_data['prediction'] - test_data['conversion_rate'])\n",
    "print('\\nmean average error: {error}'.format(error=test_data['error'].mean()))\n",
    "print('mean average error for non-zero conversion rate: {error}'.format(error=test_data.loc[test_data['conversion_rate'] > 0]['error'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also didn't improve our model.  One last thing we can try is to talk out low-coefficient features post-encoding. Lets take a look at the coefficient distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights['weights'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that there are a lot of features with a weight at or near 0.  Lets try to remove these and re-train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_to_keep = model_weights.loc[model_weights['weights'].abs() > 0.001]\n",
    "model_df = sampled_data[[col for col in feat_to_keep['feature']]]\n",
    "model_df['conversion_rate'] = sampled_data['conversion_rate']\n",
    "\n",
    "train_data = model_df.sample(frac=.7).to_dense()\n",
    "validation_data = model_df.drop(train_data.index).sample(frac=.66).to_dense()\n",
    "test_data = model_df.drop(train_data.index).drop(validation_data.index).to_dense()\n",
    "\n",
    "pd.concat([train_data['conversion_rate'], train_data.drop(['conversion_rate'], axis=1)], axis=1).to_csv('data/slim/train.csv', index=False, header=False)\n",
    "pd.concat([validation_data['conversion_rate'], validation_data.drop(['conversion_rate'], axis=1)], axis=1).to_csv('data/slim/validation.csv', index=False, header=False)\n",
    "\n",
    "bucket = 'beeswax-tmp-us-east-1'\n",
    "prefix = 'bid-models-test-data/canary/sagemaker'\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'slim/train/train.csv')).upload_file('data/slim/train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'slim/validation/validation.csv')).upload_file('data/slim/validation.csv')\n",
    "\n",
    "# drop the data files from disk, they are huge and we don't want to keep them\n",
    "os.remove('data/slim/train.csv')\n",
    "os.remove('data/slim/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri(boto3.Session().region_name, 'linear-learner')\n",
    "\n",
    "bucket = 'beeswax-tmp-us-east-1'\n",
    "prefix = 'bid-models-test-data/canary/sagemaker/slim'\n",
    "\n",
    "session = sagemaker.Session()\n",
    "ll = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.4xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=session)\n",
    "ll.set_hyperparameters(\n",
    "    feature_dim=len(train_data.columns)-1,\n",
    "    mini_batch_size=200,\n",
    "    predictor_type='regressor'\n",
    ")\n",
    "\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train/'.format(bucket, prefix), content_type='text/csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='text/csv')\n",
    "\n",
    "job_name = 'canary-canary-model-slim-{timestamp}'.format(timestamp=int(time.time()))\n",
    "ll.fit({'train': s3_input_train, 'validation': s3_input_validation}, job_name=job_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_predictor = ll.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "ll_predictor.content_type = 'text/csv'\n",
    "ll_predictor.serializer = csv_serializer\n",
    "ll_predictor.deserializer = json_deserializer\n",
    "\n",
    "def predict(data):\n",
    "    predictions = []\n",
    "    for array in data:\n",
    "        result = ll_predictor.predict(array)\n",
    "        predictions.append(result['predictions'][0]['score'])\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "test_data['prediction'] = predict(test_data.drop(['conversion_rate'], axis=1).as_matrix())\n",
    "test_data['error'] = np.abs(test_data['prediction'] - test_data['conversion_rate'])\n",
    "print('\\nmean average error: {error}'.format(error=test_data['error'].mean()))\n",
    "print('mean average error for non-zero conversion rate: {error}'.format(error=test_data.loc[test_data['conversion_rate'] > 0]['error'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, out model actually got worse.  We could continue with adding and removing different combinations of features, but let's stop here.  We'll redeploy our post-tuning model and move on to actuall deploying the Bid Model in Beeswax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri(boto3.Session().region_name, 'linear-learner')\n",
    "\n",
    "bucket = 'beeswax-tmp-us-east-1'\n",
    "prefix = 'bid-models-test-data/canary/sagemaker/sampled'\n",
    "\n",
    "session = sagemaker.Session()\n",
    "ll = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.4xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=session)\n",
    "ll.set_hyperparameters(\n",
    "    feature_dim=len(sampled_data.columns)-1,\n",
    "    predictor_type='regressor',\n",
    "    **tuned_hyper_params\n",
    ")\n",
    "\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train/'.format(bucket, prefix), content_type='text/csv')\n",
    "\n",
    "job_name = 'canary-cpi-model-sampled-{timestamp}'.format(timestamp=int(time.time()))\n",
    "ll.fit({'train': s3_input_train}, job_name=job_name) \n",
    "ll_predictor = ll.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model features\n",
    "features = list(sampled_data.columns)\n",
    "features.remove('conversion_rate')\n",
    "prod_model = {\n",
    "    'features': features,\n",
    "    'endpoint_name': job_name\n",
    "}\n",
    "\n",
    "with open('data/prod_model.json', 'w') as f:\n",
    "    f.write(json.dumps(prod_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python2",
   "language": "python",
   "name": "conda_python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
