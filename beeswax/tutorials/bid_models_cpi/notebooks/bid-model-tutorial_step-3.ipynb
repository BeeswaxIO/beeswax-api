{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a CPI Bid Model (Part 3)\n",
    "## Model Selection\n",
    "\n",
    "At this point, we've taken a look at our data and we know a few things about it:\n",
    "* our features have very skewed distributions\n",
    "* some features are likely highly correlated with one another\n",
    "* some features may have little correlation, or non-linear correlation with our dependent variable, `conversion_rate`\n",
    "\n",
    "Usually, we might try to fit many different models and see what performs the best.  However, for the sake of keeping this tutorial as brief as possible, we'll just use a linear regression model since its easy to understand and works pretty well for predicting continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "from sagemaker.analytics import TrainingJobAnalytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since SageMaker has a built in `linear learner` model, we'll go ahead and leverage that to save ourselves some time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri(boto3.Session().region_name, 'linear-learner')\n",
    "train_data = pd.read_pickle('./data/step2-train.pkl')\n",
    "\n",
    "bucket = 'beeswax-tmp-us-east-1'\n",
    "prefix = 'bid-models-test-data/canary/sagemaker'\n",
    "\n",
    "role = get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "ll = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.4xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=session)\n",
    "ll.set_hyperparameters(\n",
    "    feature_dim=len(train_data.columns)-1,\n",
    "    mini_batch_size=500,\n",
    "    predictor_type='regressor'\n",
    ")\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create the input data by referencing the files we wrote to S3 in the previous part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train/train.csv'.format(bucket, prefix), content_type='text/csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/validation.csv'.format(bucket, prefix), content_type='text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, its time to actually fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 'canary-cpi-model-{timestamp}'.format(timestamp=int(time.time()))\n",
    "ll.fit({'train': s3_input_train, 'validation': s3_input_validation}, job_name=job_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model, we want to determine how well the model performs.  We'll do this by running our test data through the model and comparing the results to the expected value.  Let's start by deploying the model so we can score against it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_predictor = ll.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll read in our test dataset and then setup our model to receive csv data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_pickle('./data/step2-test.pkl')\n",
    "\n",
    "ll_predictor.content_type = 'text/csv'\n",
    "ll_predictor.serializer = csv_serializer\n",
    "ll_predictor.deserializer = json_deserializer\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll loop loop over our test dataset to:\n",
    "* split data into mini-batches\n",
    "* convert those batches into CSV payloads\n",
    "* get predictions for each payload\n",
    "* merge the result back into our test dataset\n",
    "* calculate the MAE for our test dataset (this is the metric we will use to evaluate the model fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data):\n",
    "    predictions = []\n",
    "    for array in data:\n",
    "        result = ll_predictor.predict(array)\n",
    "        predictions.append(result['predictions'][0]['score'])\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "test_data['prediction'] = predict(test_data.drop(['conversion_rate'], axis=1).as_matrix())\n",
    "test_data['error'] = np.abs(test_data['prediction'] - test_data['conversion_rate'])\n",
    "print('mean average error: {error}'.format(error=test_data['error'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So a MAE of 0.0006 is VERY good.  One other thing to look at is the error for rows with non-zero conversion rates.  Since most of the rows have 0 conversions, we want to make sure that we are accurate where there is signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean average error for non-zero conversion rate: {error}'.format(error=test_data.loc[test_data['conversion_rate'] > 0]['error'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, worse than overall but still pretty good... we have reasonable but notably higher error for rows that actually have a conversion rate.  What does this mean?  It means we are really really good at predicting which inventory will not perform but not as good at predicting what will perform. In the next part, we'll look at ways to improve this performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python2",
   "language": "python",
   "name": "conda_python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
