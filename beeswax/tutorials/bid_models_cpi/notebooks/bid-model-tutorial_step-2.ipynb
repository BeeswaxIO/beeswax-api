{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a CPI Bid Model (Part 2)\n",
    "## Data Exploration\n",
    "Previously, in Part 1 of this tutorial, we loaded Win and Conversion logs from S3 into our notebook and did some initial cleansing and aggregation of the data.  In Part 2, we'll do some exploration of our data and prepare it for the actual training of a model.\n",
    "\n",
    "Let's start by reading in our dataframe from the last part and understanding how the features are distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import os\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.read_pickle(\"./data/step1.pkl\")\n",
    "\n",
    "# we don't really have any continuous features, so we'll conver most numeric fields to\n",
    "for column in df.select_dtypes(include=['int64','float64', 'bool']).columns:\n",
    "    if column in ['impressions', 'conversions']:\n",
    "        df[column] = df[column].astype('int64')\n",
    "        continue\n",
    "    if column in ['lat_long_present']:\n",
    "        df[column] = df[column].astype('int64')\n",
    "    df[column] = df[column].astype('object')\n",
    "\n",
    "# Frequency tables for each categorical feature\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    display(pd.crosstab(index=df[column], columns='% observations', normalize='columns'))\n",
    "\n",
    "\n",
    "# Histograms for each numeric features\n",
    "display(df.describe())\n",
    "%matplotlib inline\n",
    "hist = df.hist(bins=30, sharey=True, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of observations from this analysis:\n",
    "* it's a little difficult to tell much from the conversion and impression counts since we dont really have a metric that tells us how well a row performs.  We'll fix this by adding a \"conversion rate\" column.\n",
    "* `ad_position` is almost always \"above the fold\n",
    "* `content_coppa_flag` is empty 99% of the time and when its not empty, the value is always 0.  Empty and 0 have the same meaning here.  We'll drop this field since it won't give us much signal.\n",
    "* `video_start_delay` is similar; the value is either 0 or not present which implies 0 so we'll drop that one as well.\n",
    "\n",
    "Let's make these changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add \"conversion_rate\"\n",
    "df['conversion_rate'] = df['conversions']/df['impressions']\n",
    "\n",
    "# drop unwanted columns\n",
    "df = df.drop(['content_coppa_flag', 'video_start_delay', 'ad_position'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at how our features relate to the `conversion_rate` itself.  We'll bin the conversion rate to make the data easier to view,a nd then we'll look for interesting distributions that show us a particular attribute might be a good predictor of high conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['conversion_rate_bin'] = pd.cut(df['conversion_rate'], 10)\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "        display(pd.crosstab(index=df[column], columns=df['conversion_rate_bin'], normalize='columns'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "* most of the `app_bundle`s exhibit pretty poor conversion rates, but there are a small number that contribute to very good conversion rates.  We'll want to keep this in the model as it will likely give us a strong signal\n",
    "* ad format features (e.g. `banner_height` and `banner_width`) provide a lot of signal.  We'll keep these in as well.\n",
    "* a few specific device makes (Apple and Samsung) provide most of the signal.  Device model also provides some signal but its not as dramatic.\n",
    "* `display_manager` is interesting: applovin accounts for a significant portion of the dataset but with apploving the distribution is pretty flat. On the otherhand mopub accounts for a large portion of the high conversion rate lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['placement', 'platform_device_model', 'display_manager_ver'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation\n",
    "Now that we understand our dataset a little better, we can finish preparing it for model training.  This can include several steps:\n",
    "* Removing rows that have very few impressions to prevent them from skewing the model\n",
    "* Handle missing values: right now, all our missing values are encoded as \"-1\".  If we had numerical features, we might employ some technique to impute the values, but since all our features are categorical and \"null\" is actually a valid option on a real bid request, we will just leave the \"-1\" values and reformat them at the very end.\n",
    "* Converting categorical to numeric: In order for the model to understand our categorical features we need to encode them to numeric values.  We'll do this with [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).\n",
    "* Handling odd data distributions: many models work best with evenly distributed data, but since we don't really have numeric/continuous features we'll skip this for now.\n",
    "* Dropping unneeded columns: since we added a column for `conversion_rate`, we'll use this as the value we are trying to predict and drop `impressions` and `conversions`\n",
    "* Splitting the data: To avoid overfitting issues, we'll split the dataset into 3 groups: 70% for training, 20% for testing, 10% for validating\n",
    "\n",
    "\n",
    "Let's get started by removing low-impression rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['impressions'] > 5]\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at the distribution of conversion data after having removed the \"low count\" rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms for each numeric features\n",
    "display(df.describe())\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots()\n",
    "df['conversion_rate'].hist(ax=ax, bins=100, bottom=0.1)\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for the most part, we have very low conversion scores (usually 0), but in some cases there are very high conversion scores.  These scores are probably outliers since in the real world its very unlikely that a conversion rate would be ~100% for any combination of inventory.  Let's remove rows with out-lying conversion data.  We'll do this using z-scores to determine the number of standard deviations from the mean and then remove anything that has a z-score greater than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['z'] = np.abs(stats.zscore(df['conversion_rate']))\n",
    "df = df.loc[df['z'] < 3.0]\n",
    "df = df.drop(['z'], axis=1)\n",
    "\n",
    "display(df.describe())\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots()\n",
    "df['conversion_rate'].hist(ax=ax, bins=100, bottom=0.1)\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've removed the outliers, we'll need to encode all our categorical variables so the model can understand them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.get_dummies(df.to_sparse(), sparse=True, prefix_sep='-')\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's drop the unneeded cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = model_df.drop(['conversions', 'impressions'], axis=1)\n",
    "for col in model_df:\n",
    "    if col.startswith('conversion_rate_bin'):\n",
    "        model_df = model_df.drop([col], axis=1)\n",
    "\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = model_df.loc[:, ~model_df.columns.duplicated()]  # make sure we don't have any duplicated columns after the encoding\n",
    "print(model_df.shape)\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, split the data into our 3 cuts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = model_df.sample(frac=.7).to_dense()\n",
    "validation_data = model_df.drop(train_data.index).sample(frac=.66).to_dense()\n",
    "test_data = model_df.drop(train_data.index).drop(validation_data.index).to_dense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to export our data for model training.  To do this, we need to get it into the format SageMaker expects: dependent variable in the first column, no headers, csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([train_data['conversion_rate'], train_data.drop(['conversion_rate'], axis=1)], axis=1).to_dense().to_csv('data/train.csv', index=False, header=False)\n",
    "pd.concat([validation_data['conversion_rate'], validation_data.drop(['conversion_rate'], axis=1)], axis=1).to_dense().to_csv('data/validation.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're going to use SageMaker to train our model, we'll need to write the data to S3.  Let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'beeswax-tmp-us-east-1'\n",
    "prefix = 'bid-models-test-data/canary/sagemaker'\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('data/train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('data/validation.csv')\n",
    "\n",
    "# drop the data files from disk, they are huge and we don't want to keep them\n",
    "os.remove('data/train.csv')\n",
    "os.remove('data/validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To wrap up this part of the tutorial, we'll save our state so we can re-use it again in the next part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('data/step2.pkl')\n",
    "model_df.to_pickle('data/step2-model.pkl')\n",
    "test_data.to_pickle('data/step2-test.pkl')\n",
    "train_data.to_pickle('data/step2-train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! In the next part of this tutorial, we'll get to the fun part: training an actual model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python2",
   "language": "python",
   "name": "conda_python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
